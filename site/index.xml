<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Prem Seetharaman</title>
    <link>https://pseeth.github.io/</link>
    <description>Recent content on Prem Seetharaman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2023, Prem Seetharaman</copyright>
    <lastBuildDate>Sun, 01 Sep 2019 07:00:00 +0000</lastBuildDate>
    <atom:link href="https://pseeth.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bootstrapping the learning process for computer audition</title>
      <link>https://pseeth.github.io/posts/bootstrapping-the-learning-process-for-computer-audition/</link>
      <pubDate>Sun, 01 Sep 2019 07:00:00 +0000</pubDate>
      <guid>https://pseeth.github.io/posts/bootstrapping-the-learning-process-for-computer-audition/</guid>
      <description>I defended my thesis in September 2019! It was about learning to segment complex auditory scenes into constituent sources without access to ground truth training data. I developed a method for bootstrapping a deep learning model via primitives - hard-wired auditory grouping principles that have been observed in the mammalian auditory cortex. The primitive estimates were then used in concert with a confidence measure to train a deep audio source separation model.</description>
    </item>
    <item>
      <title>Bootstrapping speech separation from unsupervised spatial separation</title>
      <link>https://pseeth.github.io/posts/bootstrapping-speech-separation-from-unsupervised-spatial-separation/</link>
      <pubDate>Sun, 10 Feb 2019 08:00:00 +0000</pubDate>
      <guid>https://pseeth.github.io/posts/bootstrapping-speech-separation-from-unsupervised-spatial-separation/</guid>
      <description>Separating an audio scene into isolated sources is a fundamental problem in computer audition, analogous to image segmentation in visual scene analysis. Source separation systems based on deep learning are currently the most successful approaches for solving the underdetermined separation problem, where there are more sources than channels. Traditionally, such systems are trained on sound mixtures where the ground truth decomposition is already known. Since most real-world recordings do not have such a decomposition available, this limits the range of mixtures one can train on, and the range of mixtures the learned models may successfully separate.</description>
    </item>
    <item>
      <title></title>
      <link>https://pseeth.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://pseeth.github.io/about/</guid>
      <description>About I am a Senior Research Scientist at Adobe Research, working in the Audio AI Lab. I received my PhD in 2019 at Northwestern University, advised by Bryan Pardo. Afterwards, I spent some time at Descript where I worked on audio enhancement and generation. My recent work focuses on generative models for all types of audio, such as every-day sounds, music, and speech. I work at the intersection of computer audition, generative modeling, and interaction, with the goal of lowering the barrier to entry for content creation.</description>
    </item>
    <item>
      <title></title>
      <link>https://pseeth.github.io/music/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://pseeth.github.io/music/</guid>
      <description>six pieces for two voices Score&#xA;four pieces for piano Score&#xA;resonance, for solo cello Score</description>
    </item>
    <item>
      <title></title>
      <link>https://pseeth.github.io/notebooks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://pseeth.github.io/notebooks/</guid>
      <description>Jupyter Notebooks and Lectures For some papers, I create companion notebooks that I hope are useful for people delving into the work! The papers and links to their associated notebooks are below. Enjoy!&#xA;[notebook] [pdf] Seetharaman, Prem, and Zafar Rafii. “Cover Song Identification with 2D Fourier Transform Sequences.” 42nd International Conference on Acoustics, Speech, and Signal Processing, New Orleans, USA, March 5 - 9, 2017.&#xA;[notebook] [pdf] Seetharaman, Prem, and Bryan Pardo.</description>
    </item>
    <item>
      <title></title>
      <link>https://pseeth.github.io/papers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://pseeth.github.io/papers/</guid>
      <description>Thesis [pdf] [demo] Seetharaman, P. (2019). Bootstrapping the Learning Process for Computer Audition (PhD thesis). Northwestern University.&#xA;Patents Rafii, Z., &amp;amp; Seetharaman, P. (2018). Audio Identification Based on Data Structure.&#xA;Cremer, M. K., Rafii, Z., Coover, R., &amp;amp; Seetharaman, P. (2018). Automated Cover Song Identification.&#xA;Refereed Conference Papers [pdf] Liu, A., Seetharaman, P., &amp;amp; Pardo, B. (2020). Model Selection for Deep Audio Source Separation via Clustering Analysis. ArXiv Preprint ArXiv:1910.12626.</description>
    </item>
    <item>
      <title>News</title>
      <link>https://pseeth.github.io/news/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://pseeth.github.io/news/</guid>
      <description>Dec 2023 I joined Adobe Research as a Senior Research Scientist, working in the Audio AI Lab.&#xA;Jul 2023 Descript launches Regenerate (led by Rithesh Kumar), powered by the Descript Audio Codec!&#xA;Jun 2023 Released VampNet, (ISMIR 2023) a really fun (and fast) unconditional music generation model with my intern Hugo!&#xA;Jun 2023 I had a kid!&#xA;May 2023 Released the Descript Audio Codec (NeurIPS 2023), a powerful neural audio codec that can compress audio at 90x compression rate with minimal quality loss.</description>
    </item>
  </channel>
</rss>
